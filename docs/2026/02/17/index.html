<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>医疗AI学术进展 | 2026-02-16</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --color-bg: #fafafa;
            --color-surface: #ffffff;
            --color-border: #e5e7eb;
            --color-border-light: #f3f4f6;
            --color-text: #111827;
            --color-text-secondary: #4b5563;
            --color-text-muted: #6b7280;
            --color-accent: #1e40af;
            --color-accent-light: #dbeafe;
            --font-serif: 'Crimson Pro', Georgia, 'Times New Roman', serif;
            --font-sans: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: var(--font-sans);
            background: var(--color-bg);
            color: var(--color-text);
            line-height: 1.6;
            font-size: 17px;
        }

        /* Header */
        .header {
            background: var(--color-surface);
            border-bottom: 1px solid var(--color-border);
            padding: 0;
        }

        .header-top {
            max-width: 1400px;
            margin: 0 auto;
            padding: 16px 24px;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 24px;
        }

        .header-brand {
            display: flex;
            align-items: center;
            gap: 16px;
        }

        .header-logo {
            width: 48px;
            height: 48px;
            background: var(--color-accent);
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-family: var(--font-serif);
            font-size: 24px;
            font-weight: 700;
        }

        .header-title-group {}

        .header-title {
            font-family: var(--font-serif);
            font-size: 26px;
            font-weight: 700;
            color: var(--color-text);
            letter-spacing: -0.02em;
        }

        .header-subtitle {
            font-size: 14px;
            color: var(--color-text-muted);
            margin-top: 4px;
        }

        .header-meta {
            text-align: right;
        }

        .header-date {
            font-size: 14px;
            font-weight: 600;
            color: var(--color-text-secondary);
            font-family: var(--font-serif);
            font-style: italic;
        }

        .header-range {
            font-size: 13px;
            color: var(--color-text-muted);
            margin-top: 4px;
        }

        .header-total {
            font-size: 32px;
            font-weight: 700;
            color: var(--color-accent);
            margin-top: 8px;
        }

        .header-total-label {
            font-size: 12px;
            color: var(--color-text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        /* Navigation Bar */
        .nav-bar {
            background: var(--color-surface);
            border-bottom: 1px solid var(--color-border);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav-content {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 24px;
            display: flex;
            gap: 4px;
            overflow-x: auto;
        }

        .nav-item {
            padding: 12px 16px;
            font-size: 14px;
            font-weight: 500;
            color: var(--color-text-secondary);
            cursor: pointer;
            border: none;
            background: none;
            border-bottom: 2px solid transparent;
            white-space: nowrap;
            transition: all 0.15s;
        }

        .nav-item:hover {
            color: var(--color-text);
            background: var(--color-border-light);
        }

        .nav-item.active {
            color: var(--color-accent);
            border-bottom-color: var(--color-accent);
            font-weight: 600;
        }

        /* Main Content */
        .main {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px 24px 40px;
        }

        /* Stats Panel */
        .stats-panel {
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-radius: 8px;
            padding: 16px 20px;
            margin-bottom: 20px;
        }

        .stats-title {
            font-size: 12px;
            font-weight: 600;
            color: var(--color-text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 12px;
        }

        .stats-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 12px 24px;
        }

        .stat-item {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 6px 12px;
            border-radius: 4px;
            cursor: pointer;
            transition: background 0.15s;
        }

        .stat-item:hover {
            background: var(--color-border-light);
        }

        .stat-item.active {
            background: var(--color-accent-light);
        }

        .stat-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
        }

        .stat-name {
            font-size: 13px;
            color: var(--color-text-secondary);
        }

        .stat-item.active .stat-name {
            color: var(--color-accent);
            font-weight: 600;
        }

        .stat-count {
            font-size: 13px;
            font-weight: 600;
            color: var(--color-text);
            background: var(--color-border-light);
            padding: 2px 8px;
            border-radius: 12px;
            min-width: 24px;
            text-align: center;
        }

        .stat-item.active .stat-count {
            background: var(--color-accent);
            color: white;
        }

        /* Section */
        .section {
            margin-bottom: 24px;
        }

        .section-header {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 12px 16px;
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-left-width: 4px;
            border-radius: 0 8px 8px 0;
            margin-bottom: 12px;
        }

        .section-icon {
            font-size: 18px;
        }

        .section-title {
            font-family: var(--font-serif);
            font-size: 18px;
            font-weight: 700;
        }

        .section-count {
            margin-left: auto;
            font-size: 13px;
            color: var(--color-text-muted);
            font-weight: 500;
        }

        /* Papers Grid */
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 12px;
        }

        @media (max-width: 1100px) {
            .papers-grid { grid-template-columns: 1fr; }
        }

        /* Paper Card */
        .paper-card {
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-left-width: 3px;
            border-radius: 0 6px 6px 0;
            overflow: hidden;
            transition: box-shadow 0.15s, border-color 0.15s;
        }

        .paper-card:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
            border-color: #d1d5db;
        }

        .paper-header {
            padding: 14px 16px 10px;
            border-bottom: 1px solid var(--color-border-light);
        }

        .paper-category {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 8px;
        }

        .paper-title {
            font-family: var(--font-serif);
            font-size: 17px;
            font-weight: 600;
            line-height: 1.4;
            color: var(--color-text);
        }

        .paper-title a {
            color: inherit;
            text-decoration: none;
        }

        .paper-title a:hover {
            color: var(--color-accent);
            text-decoration: underline;
        }

        .paper-meta {
            display: flex;
            gap: 12px;
            padding: 8px 16px;
            font-size: 12px;
            color: var(--color-text-muted);
            border-bottom: 1px solid var(--color-border-light);
            background: #fafafa;
        }

        .paper-meta-item {
            display: flex;
            align-items: center;
            gap: 4px;
        }

        .paper-body {
            padding: 12px 16px;
        }

        .paper-summary {
            font-size: 16px;
            line-height: 1.7;
            color: var(--color-text-secondary);
            margin-bottom: 12px;
        }

        .paper-summary::before {
            content: '';
            display: inline-block;
            width: 3px;
            height: 14px;
            background: var(--color-accent);
            margin-right: 8px;
            vertical-align: middle;
        }

        /* Abstract */
        .abstract-section {
            margin-top: 12px;
        }

        .abstract-toggle {
            width: 100%;
            padding: 8px 0;
            background: none;
            border: none;
            font-size: 12px;
            font-weight: 500;
            color: var(--color-accent);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: space-between;
            transition: color 0.15s;
        }

        .abstract-toggle:hover {
            color: #1e3a8a;
        }

        .abstract-toggle-icon {
            transition: transform 0.2s;
        }

        .abstract-toggle.expanded .abstract-toggle-icon {
            transform: rotate(180deg);
        }

        .abstract-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }

        .abstract-content.expanded {
            max-height: 2000px;
        }

        .abstract-inner {
            padding: 12px 0 4px;
            border-top: 1px dashed var(--color-border);
            margin-top: 8px;
        }

        .abstract-block {
            margin-bottom: 12px;
        }

        .abstract-label {
            font-size: 11px;
            font-weight: 600;
            color: var(--color-text-muted);
            text-transform: uppercase;
            letter-spacing: 0.03em;
            margin-bottom: 6px;
        }

        .abstract-text {
            font-size: 15px;
            line-height: 1.7;
            color: var(--color-text-secondary);
        }

        /* Authors & Keywords */
        .paper-footer {
            padding: 10px 16px;
            background: #fafafa;
            border-top: 1px solid var(--color-border-light);
        }

        .paper-authors {
            font-size: 14px;
            color: var(--color-text-secondary);
            margin-bottom: 10px;
            line-height: 1.5;
        }

        .author {
            font-weight: 500;
        }

        .paper-keywords {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }

        .keyword {
            font-size: 11px;
            color: var(--color-accent);
            background: var(--color-accent-light);
            padding: 3px 10px;
            border-radius: 3px;
            font-weight: 500;
        }

        /* Paper Actions */
        .paper-actions {
            display: flex;
            gap: 8px;
            padding: 10px 16px;
            border-top: 1px solid var(--color-border-light);
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            font-size: 13px;
            font-weight: 500;
            text-decoration: none;
            border-radius: 4px;
            transition: all 0.15s;
            cursor: pointer;
            border: none;
        }

        .btn-primary {
            background: var(--color-accent);
            color: white;
        }

        .btn-primary:hover {
            background: #1e3a8a;
        }

        .btn-secondary {
            background: white;
            color: var(--color-text-secondary);
            border: 1px solid var(--color-border);
        }

        .btn-secondary:hover {
            background: var(--color-border-light);
            color: var(--color-text);
        }

        /* Footer */
        .footer {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px 24px;
            border-top: 1px solid var(--color-border);
            text-align: center;
        }

        .footer-text {
            font-size: 12px;
            color: var(--color-text-muted);
        }

        /* Hidden state */
        .hidden { display: none !important; }

        /* Responsive */
        @media (max-width: 768px) {
            .header-top {
                flex-direction: column;
                padding: 20px;
                gap: 20px;
            }
            .header-meta { text-align: left; }
            .nav-content { padding: 0 20px; }
            .main { padding: 20px; }
            .papers-grid { grid-template-columns: 1fr; }
        }


        .section-header.医疗大模型 { border-left-color: #1e40af; }
        .section-header.医疗大模型 .section-icon { color: #1e40af; }
        .paper-card[data-category="医疗大模型"] { border-left-color: #1e40af; }
        .paper-card[data-category="医疗大模型"] .paper-category { color: #1e40af; }
        .stat-item[data-filter="医疗大模型"].active .stat-dot { box-shadow: 0 0 0 3px #1e40af30; }

        .section-header.医疗数据集 { border-left-color: #047857; }
        .section-header.医疗数据集 .section-icon { color: #047857; }
        .paper-card[data-category="医疗数据集"] { border-left-color: #047857; }
        .paper-card[data-category="医疗数据集"] .paper-category { color: #047857; }
        .stat-item[data-filter="医疗数据集"].active .stat-dot { box-shadow: 0 0 0 3px #04785730; }

        .section-header.医疗智能体 { border-left-color: #7c3aed; }
        .section-header.医疗智能体 .section-icon { color: #7c3aed; }
        .paper-card[data-category="医疗智能体"] { border-left-color: #7c3aed; }
        .paper-card[data-category="医疗智能体"] .paper-category { color: #7c3aed; }
        .stat-item[data-filter="医疗智能体"].active .stat-dot { box-shadow: 0 0 0 3px #7c3aed30; }

    </style>
</head>
<body>
    <header class="header">
        <div class="header-top">
            <div class="header-brand">
                <div class="header-logo">arXiv</div>
                <div class="header-title-group">
                    <h1 class="header-title">医疗AI学术进展周报</h1>
                    <p class="header-subtitle">自动文献综述与前沿追踪</p>
                </div>
            </div>
            <div class="header-meta">
                <div class="header-date">February 17, 2026</div>
                <div class="header-range">2026-02-16 — 2026-02-16</div>
                <div class="header-total">8</div>
                <div class="header-total-label">论文</div>
            </div>
        </div>
    </header>

    <main class="main">
        <div class="stats-panel">
            <div class="stats-title">按研究领域筛选</div>
            <div class="stats-grid">
                <div class="stat-item active" data-filter="all">
                    <span class="stat-dot" style="background: var(--color-accent);"></span>
                    <span class="stat-name">全部主题</span>
                    <span class="stat-count">8</span>
                </div>
                <div class="stat-item" data-filter="医疗大模型">
                    <span class="stat-dot" style="background: #1e40af;"></span>
                    <span class="stat-name">医疗大模型</span>
                    <span class="stat-count">4</span>
                </div>
                <div class="stat-item" data-filter="医疗数据集">
                    <span class="stat-dot" style="background: #047857;"></span>
                    <span class="stat-name">医疗数据集</span>
                    <span class="stat-count">1</span>
                </div>
                <div class="stat-item" data-filter="医疗智能体">
                    <span class="stat-dot" style="background: #7c3aed;"></span>
                    <span class="stat-name">医疗智能体</span>
                    <span class="stat-count">3</span>
                </div>
            </div>
        </div>

        <section class="section">
            <div class="section-header 医疗大模型">
                <span class="section-icon" style="color: #1e40af;">◆</span>
                <h2 class="section-title">医疗大模型</h2>
                <span class="section-count">4 篇论文</span>
            </div>
            <div class="papers-grid">

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15012v1" target="_blank">Cold-Start Personalization via Training-Free Priors from Structured World Models</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15012v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出Pep框架，通过离线学习偏好关联的结构化世界模型，在线进行无需训练的贝叶斯推断来选择问题并预测完整偏好，解决了冷启动个性化中利用偏好数据结构的关键挑战，在多个领域以更少交互实现了比强化学习更高的对齐度。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">冷启动个性化（Cold-start personalization）需要在没有用户特定历史数据的情况下，通过交互推断用户偏好。核心挑战是一个路由问题：每个任务涉及数十个偏好维度，但个体用户只关心少数几个，且哪些维度重要取决于具体用户。在有限的问题预算下，无结构的提问会错失关键维度。强化学习（Reinforcement learning）是自然的表述方式，但在多轮设置中，其终端奖励未能利用偏好数据的分解式、按标准（factored, per-criterion）结构，实践中学习到的策略会退化为忽略用户响应的静态问题序列。我们提出将冷启动启发分解为离线结构学习和在线贝叶斯推断。Pep（Preference Elicitation with Priors）从完整配置文件中离线学习偏好关联的结构化世界模型（structured world model），然后在线执行无需训练（training-free）的贝叶斯推断（Bayesian inference）来选择信息丰富的问题并预测完整的偏好配置文件，包括从未询问过的维度。该框架在下游求解器之间是模块化的，并且只需要简单的信念模型。在医学、数学、社交和常识推理（commonsense reasoning）领域，Pep实现了生成响应与用户陈述偏好之间80.8%的对齐度，而强化学习为68.5%，且交互次数减少了3-5倍。当两个用户对同一问题给出不同答案时，Pep改变其后续问题的比例为39-62%，而强化学习为0-28%。Pep仅使用约1万个参数，而强化学习使用80亿个，这表明冷启动启发的瓶颈在于利用偏好数据分解结构（factored structure）的能力。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Bose A</span>, <span class="author">Li SS</span>, <span class="author">Brahman F</span>, <span class="author">Koh PW</span>, <span class="author">Du SS</span>, <span class="author">Tsvetkov Y</span>, <span class="author">等</span></div>
                <div class="paper-keywords"><span class="keyword">冷启动个性化</span><span class="keyword">结构化世界模型</span><span class="keyword">贝叶斯推断</span><span class="keyword">偏好启发</span><span class="keyword">训练无关先验</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15012v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15012v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.14926v1" target="_blank">MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.14926v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出MAC-AMP系统，一个基于大语言模型多智能体协作的闭环系统，用于多目标抗菌肽设计。该系统通过模拟同行评审和自适应强化学习框架，仅需任务描述和示例数据即可自主设计新型抗菌肽，在活性、毒性、新颖性等多个关键分子属性上实现优化，且结果具有可解释性。实验表明其性能优于现有生成模型。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">为应对全球健康威胁——抗菌素耐药性，抗菌肽（AMP）因其对抗耐药病原体的强大潜力而被探索。虽然人工智能（AI）已被用于推进AMP的发现与设计，但大多数AMP设计模型难以平衡活性、毒性和新颖性等关键目标，且使用僵化或不明确的评分方法，导致结果难以解释和优化。随着大语言模型（LLM）能力的快速演进，我们转向基于此类模型的多智能体协作（multi-agent LLMs），其在复杂科学设计场景中展现出迅速增长的潜力。基于此，我们介绍了MAC-AMP，一个用于多目标AMP设计的闭环多智能体协作（MAC）系统。该系统实现了一个完全自主的模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可设计新型AMP。我们工作的新颖性在于为AMP设计引入了一个具有跨领域可迁移性的闭环多智能体系统，它支持多目标优化，同时保持可解释性而非“黑箱”。实验表明，MAC-AMP通过有效优化多个关键分子属性的AMP生成，在抗菌活性、AMP相似性、毒性合规性和结构可靠性方面表现出优异结果，性能优于其他AMP生成模型。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Zhou G</span>, <span class="author">Janarthanan S</span>, <span class="author">Chen L</span>, <span class="author">Hu P</span></div>
                <div class="paper-keywords"><span class="keyword">抗菌肽设计</span><span class="keyword">多智能体协作</span><span class="keyword">多目标优化</span><span class="keyword">闭环系统</span><span class="keyword">可解释人工智能</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.14926v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.14926v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.14879v1" target="_blank">CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.14879v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文介绍了首个CT病灶理解多模态基准数据集CT-Bench，包含带标注的病灶图像与元数据以及多任务视觉问答对。该数据集用于评估先进多模态模型性能，并通过微调验证其临床价值，以推动AI在CT病灶分析中的应用。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">人工智能（AI）可以自动在计算机断层扫描（CT）上勾画病灶并生成放射学报告内容，但由于具有病灶级标注的公开CT数据集稀缺，进展受到限制。为弥补这一差距，我们引入了CT-Bench，这是一个首创的基准数据集，包含两个部分：一个包含来自7,795个CT研究的20,335个病灶的病灶图像和元数据集（带有边界框、描述和大小信息），以及一个包含2,850个问答对的多任务视觉问答（VQA）基准，涵盖病灶定位、描述、大小估计和属性分类。其中包含了困难负例以反映真实世界的诊断挑战。我们通过比较多种最先进的多模态模型（包括视觉语言模型和医学CLIP变体）与放射科医生评估的性能，证明了CT-Bench作为病灶分析综合基准的价值。此外，在病灶图像和元数据集上对模型进行微调，在两个部分上都带来了显著的性能提升，这凸显了CT-Bench的临床实用性。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Zhu Q</span>, <span class="author">Jin Q</span>, <span class="author">Mathai TS</span>, <span class="author">Fang Y</span>, <span class="author">Wang Z</span>, <span class="author">Yang Y</span>, <span class="author">等</span></div>
                <div class="paper-keywords"><span class="keyword">CT影像</span><span class="keyword">病灶理解</span><span class="keyword">多模态基准</span><span class="keyword">视觉问答</span><span class="keyword">人工智能辅助诊断</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.14879v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.14879v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.14733v1" target="_blank">More than Decision Support: Exploring Patients' Longitudinal Usage of Large Language Models in Real-World Healthcare-Seeking Journeys</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.14733v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本研究通过为期四周的日记研究，探讨了25名患者在真实世界医疗寻求过程中对大型语言模型的纵向使用。研究发现，患者不仅将其用作决策支持工具，更将其视为在行为、信息、情感和认知层面提供支持的动态陪伴者。同时，患者赋予LLMs多样的社会技术意义，改变了医患关系中能动性、信任和权力的传统动态。研究提出将未来的LLMs概念化为纵向边界陪伴者。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">近年来，大型语言模型（LLMs）越来越多地被用于支持患者的医疗健康寻求。虽然先前以患者为中心的研究考察了基于LLM的工具在特定健康相关任务（如信息寻求、诊断或决策支持）中的能力和体验，但现实世界实践中医疗健康固有的纵向性质尚未得到充分探索。本文通过一项为期四周、涉及25名患者的日记研究，考察了LLMs在整个医疗寻求轨迹中的作用。我们的分析表明，患者不仅将LLMs整合为简单的决策支持工具，而且将其视为动态的陪伴者，在行为、信息、情感和认知层面为他们的旅程提供支持。同时，患者主动赋予LLMs多样的社会技术意义，改变了医患关系中能动性（agency）、信任和权力的传统动态。基于这些发现，我们将未来的LLMs概念化为一种纵向边界陪伴者（longitudinal boundary companion），在整个纵向医疗寻求轨迹中持续地在患者和临床医生之间进行调解。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Large language models (LLMs) have been increasingly adopted to support patients' healthcare-seeking in recent years. While prior patient-centered studies have examined the capabilities and experience of LLM-based tools in specific health-related tasks such as information-seeking, diagnosis, or decision-supporting, the inherently longitudinal nature of healthcare in real-world practice has been underexplored. This paper presents a four-week diary study with 25 patients to examine LLMs' roles across healthcare-seeking trajectories. Our analysis reveals that patients integrate LLMs not just as simple decision-support tools, but as dynamic companions that scaffold their journey across behavioral, informational, emotional, and cognitive levels. Meanwhile, patients actively assign diverse socio-technical meanings to LLMs, altering the traditional dynamics of agency, trust, and power in patient-provider relationships. Drawing from these findings, we conceptualize future LLMs as a longitudinal boundary companion that continuously mediates between patients and clinicians throughout longitudinal healthcare-seeking trajectories.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Cao Y</span>, <span class="author">Ji Y</span>, <span class="author">Fu CY</span>, <span class="author">Dharmavaram S</span>, <span class="author">Turchioe M</span>, <span class="author">Benda NC</span>, <span class="author">等</span></div>
                <div class="paper-keywords"><span class="keyword">大型语言模型</span><span class="keyword">医疗健康寻求</span><span class="keyword">纵向研究</span><span class="keyword">边界陪伴者</span><span class="keyword">医患关系</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.14733v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.14733v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>
            </div>
        </section>

        <section class="section">
            <div class="section-header 医疗数据集">
                <span class="section-icon" style="color: #047857;">◆</span>
                <h2 class="section-title">医疗数据集</h2>
                <span class="section-count">1 篇论文</span>
            </div>
            <div class="papers-grid">

        <article class="paper-card" data-category="医疗数据集">
            <div class="paper-header">
                <div class="paper-category">医疗数据集 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.14879v1" target="_blank">CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.14879v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文介绍了首个CT病灶理解多模态基准数据集CT-Bench，包含带标注的病灶图像与元数据以及多任务视觉问答对。该数据集用于评估先进多模态模型性能，并通过微调验证其临床价值，以推动AI在CT病灶分析中的应用。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">人工智能（AI）可以自动在计算机断层扫描（CT）上勾画病灶并生成放射学报告内容，但由于具有病灶级标注的公开CT数据集稀缺，进展受到限制。为弥补这一差距，我们引入了CT-Bench，这是一个首创的基准数据集，包含两个部分：一个包含来自7,795个CT研究的20,335个病灶的病灶图像和元数据集（带有边界框、描述和大小信息），以及一个包含2,850个问答对的多任务视觉问答（VQA）基准，涵盖病灶定位、描述、大小估计和属性分类。其中包含了困难负例以反映真实世界的诊断挑战。我们通过比较多种最先进的多模态模型（包括视觉语言模型和医学CLIP变体）与放射科医生评估的性能，证明了CT-Bench作为病灶分析综合基准的价值。此外，在病灶图像和元数据集上对模型进行微调，在两个部分上都带来了显著的性能提升，这凸显了CT-Bench的临床实用性。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Zhu Q</span>, <span class="author">Jin Q</span>, <span class="author">Mathai TS</span>, <span class="author">Fang Y</span>, <span class="author">Wang Z</span>, <span class="author">Yang Y</span>, <span class="author">等</span></div>
                <div class="paper-keywords"><span class="keyword">CT影像</span><span class="keyword">病灶理解</span><span class="keyword">多模态基准</span><span class="keyword">视觉问答</span><span class="keyword">人工智能辅助诊断</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.14879v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.14879v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>
            </div>
        </section>

        <section class="section">
            <div class="section-header 医疗智能体">
                <span class="section-icon" style="color: #7c3aed;">◆</span>
                <h2 class="section-title">医疗智能体</h2>
                <span class="section-count">3 篇论文</span>
            </div>
            <div class="papers-grid">

        <article class="paper-card" data-category="医疗智能体">
            <div class="paper-header">
                <div class="paper-category">医疗智能体 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15019v1" target="_blank">Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15019v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文针对全球生物制药创新日益分散、非英语信息源增多带来的资产发现挑战，提出了一种用于药物资产搜寻的基准测试方法和一种名为Bioptic Agent的树状自学习AI智能体。该智能体在覆盖多语言、非美国中心的真实复杂查询基准测试中，相比其他主流AI研究工具，取得了显著更高的F1分数（79.7%），证明了其在实现高召回、无幻觉的深度研究方面的有效性。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">生物制药创新格局已经转变：许多新药资产现在源自美国以外，并主要通过区域性、非英语渠道披露。近期数据显示，超过85%的专利申请源自美国以外，中国占全球总量近一半；非美国的学术产出份额也在增长。行业估计中国约占全球药物研发的30%，涵盖1200多种新候选药物。在这种高风险环境中，未能发现“雷达之下”的资产会给投资者和业务拓展团队带来数十亿美元的风险，使得资产搜寻成为一场覆盖范围至关重要的竞争，其中速度和完整性决定价值。然而，当今的深度研究AI智能体在跨异构、多语言信息源实现高召回发现且无幻觉方面，仍落后于人类专家。我们提出了一种用于药物资产搜寻的基准测试方法，以及一个经过调优的、基于树状结构的自学习Bioptic Agent，旨在实现完整、无幻觉的搜寻。我们使用多语言多智能体流程构建了一个具有挑战性的完整性基准：复杂的用户查询与基本事实资产配对，这些资产大多在美国中心视野之外。为了反映真实的交易复杂性，我们从专家投资者、业务拓展和风险投资专业人士处收集了筛选查询，并将其用作先验条件来生成基准查询。对于评分，我们使用经过专家意见校准的LLM-as-judge评估。我们将Boptic Agent与Claude Opus 4.6、OpenAI GPT-5.2 Pro、Perplexity Deep Research、Gemini 3 Pro + Deep Research以及Exa Websets进行了比较。Boptic Agent实现了79.7%的F1分数，而其他工具分别为56.2%（Claude Opus 4.6）、50.6%（Gemini 3 Pro + Deep Research）、46.6%（GPT-5.2 Pro）、44.2%（Perplexity Deep Research）和26.9%（Exa Websets）。性能随着额外计算资源的增加而显著提升，这支持了更多计算能带来更好结果的观点。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface "under-the-radar" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Vinogradova A</span>, <span class="author">Vinogradov V</span>, <span class="author">Greenwood L</span>, <span class="author">Yasny I</span>, <span class="author">Kobyzev D</span>, <span class="author">Kasbekar S</span>, <span class="author">等</span></div>
                <div class="paper-keywords"><span class="keyword">药物资产搜寻</span><span class="keyword">深度研究AI智能体</span><span class="keyword">多语言信息源</span><span class="keyword">基准测试</span><span class="keyword">生物制药创新</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15019v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15019v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗智能体">
            <div class="paper-header">
                <div class="paper-category">医疗智能体 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.14926v1" target="_blank">MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.14926v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出MAC-AMP系统，一个基于大语言模型多智能体协作的闭环系统，用于多目标抗菌肽设计。该系统通过模拟同行评审和自适应强化学习框架，仅需任务描述和示例数据即可自主设计新型抗菌肽，在活性、毒性、新颖性等多个关键分子属性上实现优化，且结果具有可解释性。实验表明其性能优于现有生成模型。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">为应对全球健康威胁——抗菌素耐药性，抗菌肽（AMP）因其对抗耐药病原体的强大潜力而被探索。虽然人工智能（AI）已被用于推进AMP的发现与设计，但大多数AMP设计模型难以平衡活性、毒性和新颖性等关键目标，且使用僵化或不明确的评分方法，导致结果难以解释和优化。随着大语言模型（LLM）能力的快速演进，我们转向基于此类模型的多智能体协作（multi-agent LLMs），其在复杂科学设计场景中展现出迅速增长的潜力。基于此，我们介绍了MAC-AMP，一个用于多目标AMP设计的闭环多智能体协作（MAC）系统。该系统实现了一个完全自主的模拟同行评审-自适应强化学习框架，仅需任务描述和示例数据集即可设计新型AMP。我们工作的新颖性在于为AMP设计引入了一个具有跨领域可迁移性的闭环多智能体系统，它支持多目标优化，同时保持可解释性而非“黑箱”。实验表明，MAC-AMP通过有效优化多个关键分子属性的AMP生成，在抗菌活性、AMP相似性、毒性合规性和结构可靠性方面表现出优异结果，性能优于其他AMP生成模型。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Zhou G</span>, <span class="author">Janarthanan S</span>, <span class="author">Chen L</span>, <span class="author">Hu P</span></div>
                <div class="paper-keywords"><span class="keyword">抗菌肽设计</span><span class="keyword">多智能体协作</span><span class="keyword">多目标优化</span><span class="keyword">闭环系统</span><span class="keyword">可解释人工智能</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.14926v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.14926v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗智能体">
            <div class="paper-header">
                <div class="paper-category">医疗智能体 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.14901v1" target="_blank">Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-16
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.14901v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文针对智能体医疗系统中任务专用模型的选择问题，提出了ToolSelect方法。该方法基于注意力神经过程，根据查询和模型行为摘要自适应地从候选工具池中选择最合适的专家模型。作者构建了一个包含多种任务专用模型的胸部X光智能体环境及基准测试集ToolSelectBench，实验表明ToolSelect在四个任务族上均优于10种现有先进方法。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">任务专用模型构成了智能体医疗系统（agentic healthcare systems）的支柱，使智能体能够回答跨疾病诊断、定位和报告生成等任务的临床查询。然而，对于给定任务，单一的“最佳”模型很少存在。实际上，每个任务由多个相互竞争的专家模型（specialist models）共同服务会更好，其中不同模型在不同的数据样本上表现优异。因此，对于任何给定的查询，智能体必须从异构的工具候选池中可靠地选择正确的专家模型。为此，我们引入了ToolSelect，它通过使用任务条件选择损失（task-conditional selection loss）的一致代理（consistent surrogate）最小化采样专家工具候选的总体风险，从而自适应地学习工具（tools）的模型选择。具体来说，我们提出了一种基于注意力神经过程（Attentive Neural Process-based）的选择器，以查询和每个模型的行为摘要（behavioral summaries）为条件，在专家模型中进行选择。由于缺乏既定的测试平台，我们首次引入了一个配备多样化任务专用模型套件（17个疾病检测、19个报告生成、6个视觉定位和13个VQA）的智能体胸部X光环境，并开发了包含1448个查询的基准测试ToolSelectBench。我们的结果表明，ToolSelect在四个不同的任务族上始终优于10种SOTA方法。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Saha P</span>, <span class="author">Strong J</span>, <span class="author">Alsharid M</span>, <span class="author">Mishra D</span>, <span class="author">Noble JA</span></div>
                <div class="paper-keywords"><span class="keyword">任务专用模型</span><span class="keyword">智能体医疗系统</span><span class="keyword">模型选择</span><span class="keyword">注意力神经过程</span><span class="keyword">胸部X光环境</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.14901v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.14901v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>
            </div>
        </section>
    </main>

    <footer class="footer">
        <p class="footer-text">生成于 2026-02-17 22:32 · 数据来源：arXiv.org</p>
    </footer>

    <script>
        // Filter functionality
        const statItems = document.querySelectorAll('.stat-item');
        const sections = document.querySelectorAll('.section');

        function filterPapers(filter) {
            // Update stat items
            statItems.forEach(item => {
                item.classList.toggle('active', item.dataset.filter === filter);
            });

            // Filter cards
            document.querySelectorAll('.paper-card').forEach(card => {
                if (filter === 'all' || card.dataset.category === filter) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });

            // Show/hide sections
            sections.forEach(section => {
                const visibleCards = section.querySelectorAll('.paper-card:not(.hidden)');
                if (filter === 'all' || visibleCards.length > 0) {
                    section.classList.remove('hidden');
                } else {
                    section.classList.add('hidden');
                }
            });
        }

        statItems.forEach(item => {
            item.addEventListener('click', () => filterPapers(item.dataset.filter));
        });

        // Abstract toggle
        function toggleAbstract(btn) {
            const content = btn.nextElementSibling;
            const isExpanded = content.classList.contains('expanded');
            const textSpan = btn.querySelector('span:first-child');

            if (isExpanded) {
                content.classList.remove('expanded');
                btn.classList.remove('expanded');
                textSpan.textContent = '查看摘要';
            } else {
                content.classList.add('expanded');
                btn.classList.add('expanded');
                textSpan.textContent = '收起摘要';
            }
        }
    </script>
</body>
</html>
