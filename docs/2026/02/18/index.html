<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>医疗AI学术进展 | 2026-02-17</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --color-bg: #fafafa;
            --color-surface: #ffffff;
            --color-border: #e5e7eb;
            --color-border-light: #f3f4f6;
            --color-text: #111827;
            --color-text-secondary: #4b5563;
            --color-text-muted: #6b7280;
            --color-accent: #1e40af;
            --color-accent-light: #dbeafe;
            --font-serif: 'Crimson Pro', Georgia, 'Times New Roman', serif;
            --font-sans: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: var(--font-sans);
            background: var(--color-bg);
            color: var(--color-text);
            line-height: 1.6;
            font-size: 17px;
        }

        /* Header */
        .header {
            background: var(--color-surface);
            border-bottom: 1px solid var(--color-border);
            padding: 0;
        }

        .header-top {
            max-width: 1400px;
            margin: 0 auto;
            padding: 16px 24px;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 24px;
        }

        .header-brand {
            display: flex;
            align-items: center;
            gap: 16px;
        }

        .header-logo {
            width: 48px;
            height: 48px;
            background: var(--color-accent);
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-family: var(--font-serif);
            font-size: 24px;
            font-weight: 700;
        }

        .header-title-group {}

        .header-title {
            font-family: var(--font-serif);
            font-size: 26px;
            font-weight: 700;
            color: var(--color-text);
            letter-spacing: -0.02em;
        }

        .header-subtitle {
            font-size: 14px;
            color: var(--color-text-muted);
            margin-top: 4px;
        }

        .header-meta {
            text-align: right;
        }

        .header-date {
            font-size: 14px;
            font-weight: 600;
            color: var(--color-text-secondary);
            font-family: var(--font-serif);
            font-style: italic;
        }

        .header-range {
            font-size: 13px;
            color: var(--color-text-muted);
            margin-top: 4px;
        }

        .header-total {
            font-size: 32px;
            font-weight: 700;
            color: var(--color-accent);
            margin-top: 8px;
        }

        .header-total-label {
            font-size: 12px;
            color: var(--color-text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        /* Navigation Bar */
        .nav-bar {
            background: var(--color-surface);
            border-bottom: 1px solid var(--color-border);
            position: sticky;
            top: 0;
            z-index: 100;
        }

        .nav-content {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 24px;
            display: flex;
            gap: 4px;
            overflow-x: auto;
        }

        .nav-item {
            padding: 12px 16px;
            font-size: 14px;
            font-weight: 500;
            color: var(--color-text-secondary);
            cursor: pointer;
            border: none;
            background: none;
            border-bottom: 2px solid transparent;
            white-space: nowrap;
            transition: all 0.15s;
        }

        .nav-item:hover {
            color: var(--color-text);
            background: var(--color-border-light);
        }

        .nav-item.active {
            color: var(--color-accent);
            border-bottom-color: var(--color-accent);
            font-weight: 600;
        }

        /* Main Content */
        .main {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px 24px 40px;
        }

        /* Stats Panel */
        .stats-panel {
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-radius: 8px;
            padding: 16px 20px;
            margin-bottom: 20px;
        }

        .stats-title {
            font-size: 12px;
            font-weight: 600;
            color: var(--color-text-muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 12px;
        }

        .stats-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 12px 24px;
        }

        .stat-item {
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 6px 12px;
            border-radius: 4px;
            cursor: pointer;
            transition: background 0.15s;
        }

        .stat-item:hover {
            background: var(--color-border-light);
        }

        .stat-item.active {
            background: var(--color-accent-light);
        }

        .stat-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
        }

        .stat-name {
            font-size: 13px;
            color: var(--color-text-secondary);
        }

        .stat-item.active .stat-name {
            color: var(--color-accent);
            font-weight: 600;
        }

        .stat-count {
            font-size: 13px;
            font-weight: 600;
            color: var(--color-text);
            background: var(--color-border-light);
            padding: 2px 8px;
            border-radius: 12px;
            min-width: 24px;
            text-align: center;
        }

        .stat-item.active .stat-count {
            background: var(--color-accent);
            color: white;
        }

        /* Section */
        .section {
            margin-bottom: 24px;
        }

        .section-header {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 12px 16px;
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-left-width: 4px;
            border-radius: 0 8px 8px 0;
            margin-bottom: 12px;
        }

        .section-icon {
            font-size: 18px;
        }

        .section-title {
            font-family: var(--font-serif);
            font-size: 18px;
            font-weight: 700;
        }

        .section-count {
            margin-left: auto;
            font-size: 13px;
            color: var(--color-text-muted);
            font-weight: 500;
        }

        /* Papers Grid */
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 12px;
        }

        @media (max-width: 1100px) {
            .papers-grid { grid-template-columns: 1fr; }
        }

        /* Paper Card */
        .paper-card {
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-left-width: 3px;
            border-radius: 0 6px 6px 0;
            overflow: hidden;
            transition: box-shadow 0.15s, border-color 0.15s;
        }

        .paper-card:hover {
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
            border-color: #d1d5db;
        }

        .paper-header {
            padding: 14px 16px 10px;
            border-bottom: 1px solid var(--color-border-light);
        }

        .paper-category {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 8px;
        }

        .paper-title {
            font-family: var(--font-serif);
            font-size: 17px;
            font-weight: 600;
            line-height: 1.4;
            color: var(--color-text);
        }

        .paper-title a {
            color: inherit;
            text-decoration: none;
        }

        .paper-title a:hover {
            color: var(--color-accent);
            text-decoration: underline;
        }

        .paper-meta {
            display: flex;
            gap: 12px;
            padding: 8px 16px;
            font-size: 12px;
            color: var(--color-text-muted);
            border-bottom: 1px solid var(--color-border-light);
            background: #fafafa;
        }

        .paper-meta-item {
            display: flex;
            align-items: center;
            gap: 4px;
        }

        .paper-body {
            padding: 12px 16px;
        }

        .paper-summary {
            font-size: 16px;
            line-height: 1.7;
            color: var(--color-text-secondary);
            margin-bottom: 12px;
        }

        .paper-summary::before {
            content: '';
            display: inline-block;
            width: 3px;
            height: 14px;
            background: var(--color-accent);
            margin-right: 8px;
            vertical-align: middle;
        }

        /* Abstract */
        .abstract-section {
            margin-top: 12px;
        }

        .abstract-toggle {
            width: 100%;
            padding: 8px 0;
            background: none;
            border: none;
            font-size: 12px;
            font-weight: 500;
            color: var(--color-accent);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: space-between;
            transition: color 0.15s;
        }

        .abstract-toggle:hover {
            color: #1e3a8a;
        }

        .abstract-toggle-icon {
            transition: transform 0.2s;
        }

        .abstract-toggle.expanded .abstract-toggle-icon {
            transform: rotate(180deg);
        }

        .abstract-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }

        .abstract-content.expanded {
            max-height: 2000px;
        }

        .abstract-inner {
            padding: 12px 0 4px;
            border-top: 1px dashed var(--color-border);
            margin-top: 8px;
        }

        .abstract-block {
            margin-bottom: 12px;
        }

        .abstract-label {
            font-size: 11px;
            font-weight: 600;
            color: var(--color-text-muted);
            text-transform: uppercase;
            letter-spacing: 0.03em;
            margin-bottom: 6px;
        }

        .abstract-text {
            font-size: 15px;
            line-height: 1.7;
            color: var(--color-text-secondary);
        }

        /* Authors & Keywords */
        .paper-footer {
            padding: 10px 16px;
            background: #fafafa;
            border-top: 1px solid var(--color-border-light);
        }

        .paper-authors {
            font-size: 14px;
            color: var(--color-text-secondary);
            margin-bottom: 10px;
            line-height: 1.5;
        }

        .author {
            font-weight: 500;
        }

        .paper-keywords {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }

        .keyword {
            font-size: 11px;
            color: var(--color-accent);
            background: var(--color-accent-light);
            padding: 3px 10px;
            border-radius: 3px;
            font-weight: 500;
        }

        /* Paper Actions */
        .paper-actions {
            display: flex;
            gap: 8px;
            padding: 10px 16px;
            border-top: 1px solid var(--color-border-light);
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 8px 16px;
            font-size: 13px;
            font-weight: 500;
            text-decoration: none;
            border-radius: 4px;
            transition: all 0.15s;
            cursor: pointer;
            border: none;
        }

        .btn-primary {
            background: var(--color-accent);
            color: white;
        }

        .btn-primary:hover {
            background: #1e3a8a;
        }

        .btn-secondary {
            background: white;
            color: var(--color-text-secondary);
            border: 1px solid var(--color-border);
        }

        .btn-secondary:hover {
            background: var(--color-border-light);
            color: var(--color-text);
        }

        /* Footer */
        .footer {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px 24px;
            border-top: 1px solid var(--color-border);
            text-align: center;
        }

        .footer-text {
            font-size: 12px;
            color: var(--color-text-muted);
        }

        /* Hidden state */
        .hidden { display: none !important; }

        /* Responsive */
        @media (max-width: 768px) {
            .header-top {
                flex-direction: column;
                padding: 20px;
                gap: 20px;
            }
            .header-meta { text-align: left; }
            .nav-content { padding: 0 20px; }
            .main { padding: 20px; }
            .papers-grid { grid-template-columns: 1fr; }
        }


        .section-header.医疗大模型 { border-left-color: #1e40af; }
        .section-header.医疗大模型 .section-icon { color: #1e40af; }
        .paper-card[data-category="医疗大模型"] { border-left-color: #1e40af; }
        .paper-card[data-category="医疗大模型"] .paper-category { color: #1e40af; }
        .stat-item[data-filter="医疗大模型"].active .stat-dot { box-shadow: 0 0 0 3px #1e40af30; }

        .section-header.医疗数据集 { border-left-color: #047857; }
        .section-header.医疗数据集 .section-icon { color: #047857; }
        .paper-card[data-category="医疗数据集"] { border-left-color: #047857; }
        .paper-card[data-category="医疗数据集"] .paper-category { color: #047857; }
        .stat-item[data-filter="医疗数据集"].active .stat-dot { box-shadow: 0 0 0 3px #04785730; }

        .section-header.医疗智能体 { border-left-color: #7c3aed; }
        .section-header.医疗智能体 .section-icon { color: #7c3aed; }
        .paper-card[data-category="医疗智能体"] { border-left-color: #7c3aed; }
        .paper-card[data-category="医疗智能体"] .paper-category { color: #7c3aed; }
        .stat-item[data-filter="医疗智能体"].active .stat-dot { box-shadow: 0 0 0 3px #7c3aed30; }

    </style>
</head>
<body>
    <header class="header">
        <div class="header-top">
            <div class="header-brand">
                <div class="header-logo">arXiv</div>
                <div class="header-title-group">
                    <h1 class="header-title">医疗AI学术进展周报</h1>
                    <p class="header-subtitle">自动文献综述与前沿追踪</p>
                </div>
            </div>
            <div class="header-meta">
                <div class="header-date">February 18, 2026</div>
                <div class="header-range">2026-02-17 — 2026-02-17</div>
                <div class="header-total">7</div>
                <div class="header-total-label">论文</div>
            </div>
        </div>
    </header>

    <main class="main">
        <div class="stats-panel">
            <div class="stats-title">按研究领域筛选</div>
            <div class="stats-grid">
                <div class="stat-item active" data-filter="all">
                    <span class="stat-dot" style="background: var(--color-accent);"></span>
                    <span class="stat-name">全部主题</span>
                    <span class="stat-count">7</span>
                </div>
                <div class="stat-item" data-filter="医疗大模型">
                    <span class="stat-dot" style="background: #1e40af;"></span>
                    <span class="stat-name">医疗大模型</span>
                    <span class="stat-count">5</span>
                </div>
                <div class="stat-item" data-filter="医疗数据集">
                    <span class="stat-dot" style="background: #047857;"></span>
                    <span class="stat-name">医疗数据集</span>
                    <span class="stat-count">2</span>
                </div>
                <div class="stat-item" data-filter="医疗智能体">
                    <span class="stat-dot" style="background: #7c3aed;"></span>
                    <span class="stat-name">医疗智能体</span>
                    <span class="stat-count">0</span>
                </div>
            </div>
        </div>

        <section class="section">
            <div class="section-header 医疗大模型">
                <span class="section-icon" style="color: #1e40af;">◆</span>
                <h2 class="section-title">医疗大模型</h2>
                <span class="section-count">5 篇论文</span>
            </div>
            <div class="papers-grid">

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15811v1" target="_blank">Task-Agnostic Continual Learning for Chest Radiograph Classification</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15811v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出一种用于胸部X光分类的任务无关持续学习方法CARL-XRay，通过固定主干网络并增量添加轻量级适配器和分类头，结合原型记忆和特征回放实现稳定任务识别与适应，在持续数据流中保持高性能且无需存储原始图像。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">胸部X光分类器的临床部署需要模型能够在新数据集可用时进行更新，而无需对先前观察到的数据进行重新训练或降低已验证的性能。我们首次研究了胸部X光分类的任务增量持续学习设置，其中异构的胸部X射线数据集按顺序到达，且推理时任务标识符不可用。我们提出了一种用于胸部X光的基于适配器路由的持续学习策略（CARL-XRay），它维护一个固定的高容量主干网络，并增量分配轻量级的任务特定适配器和分类器头。一个潜在任务选择器基于任务适应的特征进行操作，并利用通过紧凑原型和特征级经验回放（feature-level experience replay）保存的当前和历史上下文。这种设计支持在顺序更新过程中实现稳定的任务识别和适应，同时避免原始图像存储。在大规模公共胸部X光数据集上的实验表明，在持续数据集摄入下，该方法具有鲁棒的性能保持和可靠的任务感知推理能力。CARL-XRay在任务未知部署下优于联合训练，实现了更高的路由准确率（75.0% vs. 62.5%），同时在具有真实任务标识符的预言机（oracle）设置下保持AUROC为0.74的竞争性诊断性能，在任务未知推理下AUROC为0.75，且使用的可训练参数显著更少。最后，所提出的框架为持续临床部署中的联合训练和重复的完全重新训练提供了一个实用的替代方案。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Kavitha MS</span>, <span class="author">Zafar A</span>, <span class="author">Muneer A</span>, <span class="author">Wu J</span></div>
                <div class="paper-keywords"><span class="keyword">任务无关持续学习</span><span class="keyword">胸部X光分类</span><span class="keyword">适配器路由</span><span class="keyword">特征回放</span><span class="keyword">原型记忆</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15811v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15811v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15740v1" target="_blank">MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15740v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出了一种基于元关系Copula的图注意力网络（MRC-GAT），用于阿尔茨海默病的可解释多模态诊断。该方法通过Copula变换对齐多模态特征，并利用关系注意力机制进行融合，在TADPOLE和NACC数据集上取得了先进的分类准确率，同时增强了模型的可解释性。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">阿尔茨海默病（AD）是一种进行性神经退行性疾病，需要早期精确诊断以提供及时的临床管理。鉴于早期诊断的至关重要性，近期研究日益关注计算机辅助诊断模型以提高精度和可靠性。然而，大多数基于图的方法仍依赖于固定的结构设计，这限制了其灵活性并阻碍了在异质患者数据上的泛化能力。为克服这些限制，本文提出了基于元关系Copula的图注意力网络（MRC-GAT），作为一种高效的多模态AD分类模型。所提出的架构将基于Copula的相似性对齐、关系注意力和节点融合整合为情景元学习的核心组件，使得包括风险因素（RF）、认知测试分数和MRI属性在内的多模态特征首先通过基于Copula的变换在公共统计空间中对齐，然后通过多关系注意力机制进行融合。根据在TADPOLE和NACC数据集上的评估，MRC-GAT模型分别达到了96.87%和92.31%的准确率，与现有诊断模型相比展示了最先进的性能。最后，该模型通过在疾病诊断的各个阶段提供可解释性，证实了其鲁棒性和适用性。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Khalvandi F</span>, <span class="author">Izadi S</span>, <span class="author">Chalechale A</span></div>
                <div class="paper-keywords"><span class="keyword">阿尔茨海默病诊断</span><span class="keyword">图注意力网络</span><span class="keyword">多模态融合</span><span class="keyword">元关系学习</span><span class="keyword">可解释性</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15740v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15740v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15677v1" target="_blank">CAMEL: An ECG Language Model for Forecasting Cardiac Events</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15677v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出了首个能够预测未来心脏事件的心电图语言模型CAMEL。它通过专门的编码器实现心电信号与文本的跨模态理解，并采用课程学习进行训练。该模型在多个任务和数据集上表现出强大的零样本性能，在ECGBench和ECGForecastBench基准测试中取得了最先进的结果。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">心电图（ECG）是心脏的电记录，对诊断心血管疾病至关重要。心电图语言模型（ELMs）最近已成为一个伴随报告生成的心电图分类的有前景的框架。然而，尽管对规划早期干预具有巨大的临床价值，现有模型无法预测未来的心脏事件。为了解决这一差距，我们提出了CAMEL，这是第一个能够在更长信号持续时间上进行推理从而实现其预测能力的ELM。我们的关键见解是一个专门的心电图编码器，它实现了心电信号与文本的跨模态理解。我们使用已建立的大语言模型（LLM）训练程序训练CAMEL，将LoRA适配与课程学习流程相结合。我们的课程包括心电图分类、指标计算和多轮对话以引发推理。CAMEL在6个任务和9个数据集上展示了强大的零样本性能，包括我们为预测心律失常而引入的新基准ECGForecastBench。CAMEL在分布内和分布外均与ELM和全监督基线模型持平或超越，在ECGBench（绝对平均增益+7.0%）以及ECGForecastBench（比全监督模型高+12.4%，比零样本ELMs高+21.1%）上取得了最先进（SOTA）的结果。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Electrocardiograms (ECG) are electrical recordings of the heart that are critical for diagnosing cardiovascular conditions. ECG language models (ELMs) have recently emerged as a promising framework for ECG classification accompanied by report generation. However, current models cannot forecast future cardiac events despite the immense clinical value for planning earlier intervention. To address this gap, we propose CAMEL, the first ELM that is capable of inference over longer signal durations which enables its forecasting capability. Our key insight is a specialized ECG encoder which enables cross-understanding of ECG signals with text. We train CAMEL using established LLM training procedures, combining LoRA adaptation with a curriculum learning pipeline. Our curriculum includes ECG classification, metrics calculations, and multi-turn conversations to elicit reasoning. CAMEL demonstrates strong zero-shot performance across 6 tasks and 9 datasets, including ECGForecastBench, a new benchmark that we introduce for forecasting arrhythmias. CAMEL is on par with or surpasses ELMs and fully supervised baselines both in- and out-of-distribution, achieving SOTA results on ECGBench (+7.0% absolute average gain) as well as ECGForecastBench (+12.4% over fully supervised models and +21.1% over zero-shot ELMs).</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Velingker N</span>, <span class="author">Solko-Breslin A</span>, <span class="author">Keoliya M</span>, <span class="author">Choi S</span>, <span class="author">Xin J</span>, <span class="author">Marathe A</span>, <span class="author">等</span></div>
                <div class="paper-keywords"><span class="keyword">心电图语言模型</span><span class="keyword">心脏事件预测</span><span class="keyword">心律失常</span><span class="keyword">零样本学习</span><span class="keyword">课程学习</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15677v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15677v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15650v1" target="_blank">Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15650v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出概念增强多模态检索增强生成框架，通过将视觉表征分解为可解释的临床概念并与多模态检索增强生成结合，统一提升放射学报告生成的可解释性和事实准确性。实验表明该方法在多个数据集和模型上均优于传统基线，打破了可解释性与性能之间的权衡假设。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">通过视觉语言模型进行放射学报告生成有望减轻文档负担、提高报告一致性并加速临床工作流程。然而，其临床采用仍因缺乏可解释性以及倾向于产生与影像证据不符的幻觉发现而受到限制。现有研究通常将可解释性和准确性视为独立目标，基于概念的可解释性技术主要关注透明度，而检索增强生成方法则通过外部检索来确保事实依据。我们提出了概念增强多模态检索增强生成，这是一个统一的框架，它将视觉表征分解为可解释的临床概念，并将其与多模态RAG集成。该方法利用丰富的上下文提示进行RRG，从而提高了可解释性和事实准确性。在MIMIC-CXR和IU X-Ray数据集上，针对多种VLM架构、训练方案和检索配置进行的实验表明，在临床准确性指标和标准NLP指标上，该方法均优于传统RAG和仅基于概念的基线方法。这些结果挑战了可解释性与性能之间存在权衡的假设，表明透明的视觉概念可以增强而非损害医学VLM的诊断准确性。我们的模块化设计将可解释性分解为视觉透明度和结构化语言模型调节，为迈向临床可信赖的AI辅助放射学提供了一条有原则的途径。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Salmè M</span>, <span class="author">Siciliano F</span>, <span class="author">Silvestri F</span>, <span class="author">Soda P</span>, <span class="author">Sicilia R</span>, <span class="author">Guarrasi V</span></div>
                <div class="paper-keywords"><span class="keyword">放射学报告生成</span><span class="keyword">多模态检索增强生成</span><span class="keyword">可解释性人工智能</span><span class="keyword">临床概念增强</span><span class="keyword">视觉语言模型</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15650v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15650v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗大模型">
            <div class="paper-header">
                <div class="paper-category">医疗大模型 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15783v1" target="_blank">Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15783v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出使用可扩展图变换器对全切片图像构建的细胞图进行分类，以解决皮肤鳞状细胞癌中健康与肿瘤上皮细胞形态相似、难以区分的问题。实验表明，图变换器模型SGFormer和DIFFormer在分类准确率上优于基于图像块的最佳方法，并证明了结合形态、纹理特征及非上皮细胞类别等上下文信息的重要性。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">癌症患者的全切片图像（Whole-slide images, WSIs）包含丰富信息，可用于医学诊断或跟踪治疗进展。为实现自动化分析，基于卷积神经网络和视觉变换器（Vision Transformers）的多种深度学习方法已被开发出来，并在分割和分类任务中取得了强劲性能。然而，由于WSIs尺寸大且细胞组织结构复杂，这些模型依赖于基于图像块（patch-based）的表示，丢失了关键的组织层面上下文信息。我们提出在完整的WSI细胞图上使用可扩展图变换器（scalable Graph Transformers）进行分类。我们在一个具有挑战性的任务上评估了该方法：皮肤鳞状细胞癌（cutaneous squamous cell carcinoma, cSCC）中健康与肿瘤上皮细胞的分类，其中两种细胞类型表现出非常相似的形态，因此基于图像的方法难以区分。我们首先在单个WSI上比较了基于图像和基于图的方法。图变换器模型SGFormer和DIFFormer在3折交叉验证中分别达到了85.2 ± 1.5（±标准误）和85.1 ± 2.5的平衡准确率，而最佳的基于图像方法达到了81.2 ± 3.0。通过评估几种节点特征配置，我们发现最具信息量的表示结合了形态和纹理特征以及非上皮细胞的细胞类别，这凸显了周围细胞环境的重要性。随后，我们将工作扩展到使用来自多个患者的多个WSI进行训练。为了解决基于图像模型的计算限制，我们从每张图像中提取四个2560 × 2560像素的图像块并将其转换为图。在此设置下，DIFFormer实现了83.6 ± 1.9的平衡准确率（3折交叉验证），而最先进的基于图像模型CellViT256达到了78.1 ± 0.5。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Sancéré L</span>, <span class="author">Moreau N</span>, <span class="author">Bozek K</span></div>
                <div class="paper-keywords"><span class="keyword">图神经网络</span><span class="keyword">皮肤鳞状细胞癌</span><span class="keyword">全切片图像</span><span class="keyword">细胞分类</span><span class="keyword">可扩展图变换器</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15783v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15783v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>
            </div>
        </section>

        <section class="section">
            <div class="section-header 医疗数据集">
                <span class="section-icon" style="color: #047857;">◆</span>
                <h2 class="section-title">医疗数据集</h2>
                <span class="section-count">2 篇论文</span>
            </div>
            <div class="papers-grid">

        <article class="paper-card" data-category="医疗数据集">
            <div class="paper-header">
                <div class="paper-category">医疗数据集 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15595v1" target="_blank">Multi-Objective Coverage via Constraint Active Search</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15595v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文提出多目标覆盖问题，旨在通过少量代表性样本覆盖可行的多目标空间，以加速药物发现等科学进程。针对现有方法不足，作者提出MOC-CAS算法，利用基于置信上界的采集函数和高斯过程后验预测选择样本，并在SARS-CoV-2和癌症蛋白质数据集上验证了其优越性能。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">本文提出了新的多目标覆盖问题，其目标是识别一小部分代表性样本，其预测结果能广泛覆盖可行的多目标空间。这个问题在许多关键的现实应用中非常重要，例如药物发现和材料设计，因为这个代表性集合的评估速度比整个可行集快得多，从而显著加速科学发现过程。现有工作无法直接应用，因为它们要么关注样本空间覆盖，要么关注以帕累托前沿为目标的多目标优化。然而，化学多样性样本通常产生相同的目标轮廓，并且安全约束通常定义在目标上。为了解决这个MOC问题，我们提出了一种新颖的搜索算法MOC-CAS，它采用基于置信上界的采集函数，在高斯过程后验预测的指导下选择乐观样本。为了实现高效优化，我们开发了硬可行性测试的平滑松弛方法，并推导出一个近似优化器。与竞争基线相比，我们的MOC-CAS在针对SARS-CoV-2和癌症的大规模蛋白质靶点数据集上经验性地实现了优越的性能，每个数据集都基于源自SMILES的特征的五个目标进行了评估。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">In this paper, we formulate the new multi-objective coverage (MOC) problem where our goal is to identify a small set of representative samples whose predicted outcomes broadly cover the feasible multi-objective space. This problem is of great importance in many critical real-world applications, e.g., drug discovery and materials design, as this representative set can be evaluated much faster than the whole feasible set, thus significantly accelerating the scientific discovery process. Existing works cannot be directly applied as they either focus on sample space coverage or multi-objective optimization that targets the Pareto front. However, chemically diverse samples often yield identical objective profiles, and safety constraints are usually defined on the objectives. To solve this MOC problem, we propose a novel search algorithm, MOC-CAS, which employs an upper confidence bound-based acquisition function to select optimistic samples guided by Gaussian process posterior predictions. For enabling efficient optimization, we develop a smoothed relaxation of the hard feasibility test and derive an approximate optimizer. Compared to the competitive baselines, we show that our MOC-CAS empirically achieves superior performances across large-scale protein-target datasets for SARS-CoV-2 and cancer, each assessed on five objectives derived from SMILES-based features.</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Siam ZS</span>, <span class="author">Liu X</span>, <span class="author">Liu C</span></div>
                <div class="paper-keywords"><span class="keyword">多目标覆盖</span><span class="keyword">约束主动搜索</span><span class="keyword">高斯过程</span><span class="keyword">药物发现</span><span class="keyword">材料设计</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15595v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15595v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>

        <article class="paper-card" data-category="医疗数据集">
            <div class="paper-header">
                <div class="paper-category">医疗数据集 · cs.AI</div>
                <h3 class="paper-title"><a href="https://arxiv.org/abs/2602.15504v1" target="_blank">Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit</a></h3>
            </div>
            <div class="paper-meta">
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
                    2026-02-17
                </span>
                <span class="paper-meta-item">
                    <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"></path><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"></path></svg>
                    arXiv:2602.15504v1
                </span>
            </div>
            <div class="paper-body">
                <p class="paper-summary">本文首次在自然语言处理领域提出期望检测任务，并以医疗领域为例，构建了RedHOTExpect语料库来研究Reddit平台上患者讨论的治疗期望。研究通过大语言模型自动标注并分析了期望的表达模式与内容，发现患者更倾向于讨论积极预期，且不同疾病类型的表达存在差异。</p>
                
                <div class="abstract-section">
                    <button class="abstract-toggle" onclick="toggleAbstract(this)">
                        <span>查看摘要</span>
                        <span class="abstract-toggle-icon">▼</span>
                    </button>
                    <div class="abstract-content">
                        <div class="abstract-inner">
                            <div class="abstract-block">
                            <div class="abstract-label">中文摘要</div>
                            <div class="abstract-text">患者对其治疗的期望对治疗的成功有重大影响。虽然主要在临床环境中进行研究，但像医学subreddit这样的在线患者平台可能包含补充性见解：即患者觉得不必要或不方便在其他地方分享的治疗期望。尽管如此，尚无研究探讨用户在线讨论何种期望以及如何表达它们。这很可能是因为期望尚未在自然语言处理（NLP）中被研究过。因此，我们引入了期望检测（Expectation Detection）任务，认为期望与许多应用相关，包括意见挖掘（opinion mining）和产品设计。随后，我们提出了一个医学领域的案例研究，其中期望的提取尤为重要。我们贡献了RedHOTExpect，一个用于在此背景下研究期望的Reddit帖子语料库（4.5K个帖子）。我们使用一个大语言模型（LLM）对数据进行银标注（silver-label），并手动验证其质量（标签准确率约78%）。基于此，我们分析了哪些语言模式表征了期望，并探讨了患者期望什么以及为什么。我们发现，与心理健康背景相比，乐观和积极主动的框架（proactive framing）在关于身体或治疗相关疾病的帖子中更为明显，并且在我们的数据集中，患者主要讨论的是益处而非负面结果。RedHOTExpect语料库可从 https://www.ims.uni-stuttgart.de/data/RedHOTExpect 获取。</div>
                        </div><div class="abstract-block">
                            <div class="abstract-label">英文摘要</div>
                            <div class="abstract-text">Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect</div>
                        </div>
                        </div>
                    </div>
                </div>
        
            </div>
            <div class="paper-footer">
                <div class="paper-authors"><span class="author">Velutharambath A</span>, <span class="author">Wührl A</span></div>
                <div class="paper-keywords"><span class="keyword">期望检测</span><span class="keyword">自然语言处理</span><span class="keyword">医疗文本挖掘</span><span class="keyword">Reddit语料库</span><span class="keyword">患者期望</span></div>
            </div>
            <div class="paper-actions">
                <a href="https://arxiv.org/pdf/2602.15504v1" target="_blank" class="btn btn-primary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                    PDF
                </a>
                <a href="https://arxiv.org/abs/2602.15504v1" target="_blank" class="btn btn-secondary">
                    <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><line x1="2" y1="12" x2="22" y2="12"></line><path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path></svg>
                    arXiv
                </a>
            </div>
        </article>
            </div>
        </section>

        <section class="section">
            <div class="section-header 医疗智能体">
                <span class="section-icon" style="color: #7c3aed;">◆</span>
                <h2 class="section-title">医疗智能体</h2>
                <span class="section-count">0 篇论文</span>
            </div>
            <div class="papers-grid">
            </div>
        </section>
    </main>

    <footer class="footer">
        <p class="footer-text">生成于 2026-02-18 22:34 · 数据来源：arXiv.org</p>
    </footer>

    <script>
        // Filter functionality
        const statItems = document.querySelectorAll('.stat-item');
        const sections = document.querySelectorAll('.section');

        function filterPapers(filter) {
            // Update stat items
            statItems.forEach(item => {
                item.classList.toggle('active', item.dataset.filter === filter);
            });

            // Filter cards
            document.querySelectorAll('.paper-card').forEach(card => {
                if (filter === 'all' || card.dataset.category === filter) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });

            // Show/hide sections
            sections.forEach(section => {
                const visibleCards = section.querySelectorAll('.paper-card:not(.hidden)');
                if (filter === 'all' || visibleCards.length > 0) {
                    section.classList.remove('hidden');
                } else {
                    section.classList.add('hidden');
                }
            });
        }

        statItems.forEach(item => {
            item.addEventListener('click', () => filterPapers(item.dataset.filter));
        });

        // Abstract toggle
        function toggleAbstract(btn) {
            const content = btn.nextElementSibling;
            const isExpanded = content.classList.contains('expanded');
            const textSpan = btn.querySelector('span:first-child');

            if (isExpanded) {
                content.classList.remove('expanded');
                btn.classList.remove('expanded');
                textSpan.textContent = '查看摘要';
            } else {
                content.classList.add('expanded');
                btn.classList.add('expanded');
                textSpan.textContent = '收起摘要';
            }
        }
    </script>
</body>
</html>
